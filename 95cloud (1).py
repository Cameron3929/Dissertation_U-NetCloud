# -*- coding: utf-8 -*-
"""95cloud

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sm96euQjUgCSWqbKasiZJXBt7W_k1jMC
"""

from google.colab import drive
drive.mount('/content/drive')

# Set root path to your freshly uploaded folder
root = "/content/drive/MyDrive/95Clouds"

# ‚îÄ‚îÄ‚îÄ Install dependencies ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
!pip install torch torchvision pillow
!pip install onnx
# ‚îÄ‚îÄ‚îÄ Imports ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ‚îÄ‚îÄ‚îÄ Essential Python + PyTorch ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import os
import random
import numpy as np
from PIL import Image
from tqdm import tqdm
import matplotlib.pyplot as plt

# Torch core
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split
from torch.optim.lr_scheduler import ReduceLROnPlateau

# ‚îÄ‚îÄ‚îÄ Scikit-learn Metrics (used in Step 5 and threshold sweep) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from sklearn.metrics import (
    jaccard_score,
    accuracy_score,
    precision_score,
    recall_score)

# ‚îÄ‚îÄ‚îÄ For optional Telegram notification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import requests
# ‚îÄ‚îÄ‚îÄ (Optional) For test sound in notebook ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from IPython.display import Audio, display
# For saving predictions
from PIL import Image
import pandas as pd
import requests
def send_telegram(message):
    token = "7965666781:AAEHIBqA_Fh6jZ4Fs6L1gJ1hVqfYe-_lqSo"
    chat_id = "5117277537"
    url = f"https://api.telegram.org/bot{token}/sendMessage"
    data = {"chat_id": chat_id, "text": message}
    requests.post(url, data=data)

# ‚îÄ‚îÄ‚îÄ 1) Build a 4‚Äëin ‚Üí 1‚Äëout U‚ÄëNet + Losses ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# a) DoubleConv block
class DoubleConv(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True),)
    def forward(self, x):
        return self.net(x)

# b) U‚ÄëNet
class UNet4(nn.Module):
    def __init__(self, in_ch=4, base_c=32):
        super().__init__()
        # encoder
        self.d1 = DoubleConv(in_ch,      base_c)
        self.d2 = DoubleConv(base_c,     base_c*2)
        self.d3 = DoubleConv(base_c*2,   base_c*4)
        self.d4 = DoubleConv(base_c*4,   base_c*8)
        # bottleneck
        self.b  = DoubleConv(base_c*8,   base_c*16)
        # decoder
        self.u4 = nn.ConvTranspose2d(base_c*16, base_c*8, 2, stride=2)
        self.dc4= DoubleConv(base_c*16,  base_c*8)
        self.u3 = nn.ConvTranspose2d(base_c*8,  base_c*4, 2, stride=2)
        self.dc3= DoubleConv(base_c*8,   base_c*4)
        self.u2 = nn.ConvTranspose2d(base_c*4,  base_c*2, 2, stride=2)
        self.dc2= DoubleConv(base_c*4,   base_c*2)
        self.u1 = nn.ConvTranspose2d(base_c*2,  base_c,   2, stride=2)
        self.dc1= DoubleConv(base_c*2,   base_c)
        # final conv
        self.out= nn.Conv2d(base_c, 1, 1)
        self.pool= nn.MaxPool2d(2)

    def forward(self, x):
        d1 = self.d1(x)
        d2 = self.d2(self.pool(d1))
        d3 = self.d3(self.pool(d2))
        d4 = self.d4(self.pool(d3))
        b  = self.b(self.pool(d4))
        u4 = self.u4(b)
        c4 = self.dc4(torch.cat([u4, d4], dim=1))
        u3 = self.u3(c4)
        c3 = self.dc3(torch.cat([u3, d3], dim=1))
        u2 = self.u2(c3)
        c2 = self.dc2(torch.cat([u2, d2], dim=1))
        u1 = self.u1(c2)
        c1 = self.dc1(torch.cat([u1, d1], dim=1))
        return torch.sigmoid(self.out(c1))

# c) Dice & BCE+Dice Loss
class DiceLoss(nn.Module):
    def __init__(self, smooth=1e-6):
        super().__init__(); self.smooth = smooth
    def forward(self, pred, target):
        pred   = pred.view(-1)
        target = target.view(-1).float()
        inter  = (pred * target).sum()
        return 1 - (2*inter + self.smooth) / (pred.sum() + target.sum() + self.smooth)

class BCEDiceLoss(nn.Module):
    def __init__(self, bce_weight=0.3):
        super().__init__()
        self.bce  = nn.BCELoss()
        self.dice = DiceLoss()
        self.w    = bce_weight
    def forward(self, pred, target):
        b = self.bce(pred, target.float())
        d = self.dice(pred, target)
        return self.w * b + (1-self.w) * d
        send_telegram("‚úÖ step 1 completed!")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# 1) Re‚Äëcreate model & load weights
model = UNet4(in_ch=4, base_c=32).to(device)
model.load_state_dict(torch.load("/content/drive/MyDrive/95Clouds/best_unet_model.pth"))
model.eval()
# Re-define loss function after a runtime reset
criterion = BCEDiceLoss(bce_weight=0.3)

model = UNet4(in_ch=4, base_c=32).to("cuda" if torch.cuda.is_available() else "cpu")
model.load_state_dict(torch.load("/content/drive/MyDrive/95Clouds/best_unet_model.pth"))
model.eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ‚îÄ‚îÄ‚îÄ 2) EnhancedAugmentation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class EnhancedAugment:
    """
    1) Random flips & 90¬∞ rotations
    2) Occasional contrast shift
    """
    def __call__(self, image, mask):
        # Horizontal flip
        if random.random() > 0.5:
            image = np.flip(image, axis=2).copy()
            mask  = np.flip(mask,  axis=1).copy()
        # Vertical flip
        if random.random() > 0.5:
            image = np.flip(image, axis=1).copy()
            mask  = np.flip(mask,  axis=0).copy()
        # Random 90¬∞ rotations
        k = random.choice([0,1,2,3])
        if k:
            image = np.rot90(image, k, axes=(1,2)).copy()
            mask  = np.rot90(mask,  k, axes=(0,1)).copy()
        # Random contrast shift (20% of time)
        if random.random() > 0.8:
            factor = 0.9 + 0.2*random.random()
            image  = np.clip(image * factor, 0, 1)
        return image, mask
        send_telegram("‚úÖ step 2 completed!")

# ‚îÄ‚îÄ‚îÄ 3) Build Dataset + DataLoader ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# a) Dataset class for train patches only
class Landsat38TrainPatches(Dataset):
    def __init__(self, root_dir, transform=None):
        base = os.path.join(root_dir, "38-Cloud_training")
        # band dirs
        self.band_dirs = {b: os.path.join(base, f"train_{b}")
                         for b in ("blue","green","red","nir")}
        for p in self.band_dirs.values():
            if not os.path.isdir(p):
                raise FileNotFoundError(f"Missing {p}")
        # mask dir
        self.mask_dir = os.path.join(base, "train_gt")
        if not os.path.isdir(self.mask_dir):
            raise FileNotFoundError(f"Missing {self.mask_dir}")
        # common suffixes
        sets = []
        for b,p in self.band_dirs.items():
            names = [f for f in os.listdir(p) if f.startswith(f"{b}_patch_")]
            sets.append({n.split(f"{b}_patch_")[1] for n in names})
        self.files = sorted(set.intersection(*sets))
        self.transform = transform
        print(f"‚Üí Loaded {len(self.files)} patches from {base}")

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        suffix = self.files[idx]
        # load bands
        bands = []
        for b,p in self.band_dirs.items():
            img = Image.open(os.path.join(p, f"{b}_patch_{suffix}"))
            bands.append(np.array(img, dtype=np.float32))
        image = np.stack(bands, axis=0) / 10000.0
        # load mask & binarize
        mask = np.array(Image.open(os.path.join(self.mask_dir, f"gt_patch_{suffix}")), dtype=np.int64)
        mask = (mask > 0).astype(np.int64)
        # apply augment
        if self.transform:
            image, mask = self.transform(image, mask)
        return torch.from_numpy(image), torch.from_numpy(mask)

# b) Instantiate & split
root = "/content/drive/MyDrive/95Clouds"
full_ds = Landsat38TrainPatches(root, transform=EnhancedAugment())

# 80/20 split
import torch
from torch.utils.data import random_split
n = len(full_ds)
n_train = int(n*0.8)
n_val   = n - n_train
train_ds, val_ds = random_split(full_ds, [n_train, n_val], generator=torch.Generator().manual_seed(42))

# c) DataLoaders
train_loader = DataLoader(train_ds, batch_size=8, shuffle=True,  num_workers=2, pin_memory=True)
val_loader   = DataLoader(val_ds,   batch_size=8, shuffle=False, num_workers=2, pin_memory=True)

print(f"‚úÖ Train: {len(train_ds)}, Val: {len(val_ds)} patches")
imgs, masks = next(iter(train_loader))
print("Batch shapes:", imgs.shape, masks.shape)
send_telegram("‚úÖ step 3 completed!")

# ‚îÄ‚îÄ‚îÄ 4) Model, Optimizer, Scheduler + Training ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# a) Device & model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model  = UNet4(in_ch=4, base_c=32).to(device)

# b) Loss & optimizer
criterion = BCEDiceLoss(bce_weight=0.3)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# c) Scheduler
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)

# d) Training loop
drive_ckpt = "/content/drive/MyDrive/95Clouds/best_unet_model.pth"
best_val   = float('inf')
num_epochs = 5

for epoch in range(1, num_epochs+1):
    # ‚Äî Train ‚Äî
    model.train()
    train_loss = 0
    for imgs, gts in tqdm(train_loader, desc=f"Epoch {epoch} [Train]"):
        imgs = imgs.to(device)
        gts  = gts.unsqueeze(1).to(device).float()  # [B,1,H,W]
        assert gts.min() >= 0 and gts.max() <= 1
        preds = model(imgs)
        loss  = criterion(preds, gts)
        optimizer.zero_grad(); loss.backward(); optimizer.step()
        train_loss += loss.item() * imgs.size(0)
    train_loss /= len(train_loader.dataset)

    # ‚Äî Validate ‚Äî
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for imgs, gts in tqdm(val_loader, desc=f"Epoch {epoch} [ Val ]"):
            imgs = imgs.to(device)
            gts  = gts.unsqueeze(1).to(device).float()
            preds= model(imgs)
            val_loss += criterion(preds, gts).item() * imgs.size(0)
    val_loss /= len(val_loader.dataset)

    # ‚Äî Scheduler & checkpoint ‚Äî
    scheduler.step(val_loss)
    print(f"\nEpoch {epoch}: Train={train_loss:.4f}, Val={val_loss:.4f}")
    if val_loss < best_val:
        best_val = val_loss
        torch.save(model.state_dict(), drive_ckpt)
        print(f"  ‚Üí New best saved to {drive_ckpt}")
        send_telegram("‚úÖ step 4 completed!")

"""rerun epochs cell"""

# ‚îÄ‚îÄ‚îÄ Resume Training: epochs 21‚Äë30 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
start_epoch = 61
end_epoch   = 62       # change to 40 if you want even more

# 1) Re‚Äëcreate model & load weights
model = UNet4(in_ch=4, base_c=32).to(device)
model.load_state_dict(torch.load("/content/drive/MyDrive/95Clouds/best_unet_model.pth"))

# 2) Fresh optimizer with a lower LR (scheduler likely keeps 1e‚Äë3 so reduce manually)
optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)

# 3) Scheduler (same settings)
from torch.optim.lr_scheduler import ReduceLROnPlateau
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)

best_val = float('inf')

for epoch in range(start_epoch, end_epoch+1):
    # --- Train ---
    model.train()
    train_loss = 0
    for imgs, gts in tqdm(train_loader, desc=f"Epoch {epoch} [Train]"):
        imgs = imgs.to(device)
        gts  = gts.unsqueeze(1).to(device).float()
        preds = model(imgs)
        loss  = criterion(preds, gts)
        optimizer.zero_grad(); loss.backward(); optimizer.step()
        train_loss += loss.item() * imgs.size(0)
    train_loss /= len(train_loader.dataset)

    # --- Validate ---
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for imgs, gts in tqdm(val_loader, desc=f"Epoch {epoch} [ Val ]"):
            imgs = imgs.to(device)
            gts  = gts.unsqueeze(1).to(device).float()
            val_loss += criterion(model(imgs), gts).item() * imgs.size(0)
    val_loss /= len(val_loader.dataset)

    scheduler.step(val_loss)
    print(f"\nEpoch {epoch}: Train={train_loss:.4f}, Val={val_loss:.4f}")

    if val_loss < best_val:
        best_val = val_loss
        torch.save(model.state_dict(), "/content/drive/MyDrive/95Clouds/best_unet_model.pth")
        print("  ‚Üí New best model saved!")
        send_telegram("‚úÖ 60 epochs finished!")

# ‚Ä¶ earlier code unchanged ‚Ä¶
# ‚îÄ‚îÄ‚îÄ Cell 5: Eval + sample viz (skip bad arrays) ‚îÄ‚îÄ‚îÄ

import matplotlib.pyplot as plt

# 1Ô∏è‚É£ Threshold
thresh = best_threshold if 'best_threshold' in globals() else 0.5

# 2Ô∏è‚É£ Collect preds & gts
all_pred, all_gt = [], []
model.eval()
with torch.no_grad():
    for imgs, gts in tqdm(val_loader, desc='Predicting'):
        imgs, gts = imgs.to(device), gts.to(device)
        probs      = model(imgs)
        pred_bin   = (probs > thresh).long()
        all_pred.append(pred_bin.cpu())
        all_gt.append(gts.cpu())

all_pred = torch.cat(all_pred).numpy().ravel()
all_gt   = torch.cat(all_gt).numpy().ravel()

# 3Ô∏è‚É£ Compute metrics
tp = ((all_pred==1)&(all_gt==1)).sum()
tn = ((all_pred==0)&(all_gt==0)).sum()
fp = ((all_pred==1)&(all_gt==0)).sum()
fn = ((all_pred==0)&(all_gt==1)).sum()
eps=1e-6

precision = tp/(tp+fp+eps)
recall    = tp/(tp+fn+eps)
dice_m    = 2*tp/(2*tp+fp+fn+eps)
iou_m     = tp/(tp+fp+fn+eps)
accuracy  = (tp+tn)/(tp+tn+fp+fn+eps)

print("\n‚Äî Evaluation Metrics ‚Äî")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"Dice:      {dice_m:.4f}")
print(f"IoU:       {iou_m:.4f}")
print(f"Accuracy:  {accuracy:.4f}")
# 4Ô∏è‚É£ Visualize up to 50 examples
shown, max_show = 0, 50

for imgs, gts in val_loader:
    imgs, gts = imgs.to(device), gts.to(device)
    with torch.no_grad():
        preds = (model(imgs) > thresh).long()

    bs, C, H, W = imgs.shape
    band_names  = ['Red','Green','Blue','NIR']
    n_bands     = min(C,4)

    for i in range(bs):
        if shown >= max_show: break

        img_np  = imgs[i].cpu().numpy()      # (C,H,W)
        pred_np = preds[i,0].cpu().numpy()   # (H,W)
        gt_np   = gts[i].cpu().numpy()       # maybe (1,H,W)

        # Squeeze any singleton channel
        if gt_np.ndim==3 and gt_np.shape[0]==1:
            gt_np = gt_np[0]

        arrays = [img_np[b] for b in range(n_bands)] + [pred_np, gt_np]
        titles = band_names[:n_bands] + ['Prediction','Ground Truth']

        fig, axes = plt.subplots(1, len(arrays), figsize=(3*len(arrays),3))
        for ax, arr, ttl in zip(axes, arrays, titles):
            if arr.ndim == 2:
                ax.imshow(arr, cmap='gray')
            else:
                ax.text(0.5,0.5,'?', ha='center', va='center', fontsize=20)
                ax.set_facecolor('lightgray')
            ax.set_title(ttl)
            ax.axis('off')

        plt.tight_layout()
        plt.show()

        shown += 1
    if shown >= max_show: break

# ‚îÄ‚îÄ‚îÄ 5) Evaluation Metrics + Filtered Visualizations ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 1) Load the best model
model.load_state_dict(torch.load("/content/drive/MyDrive/95Clouds/best_unet_model.pth"))
model.eval()

all_preds = []
all_labels = []

# 2) Run through the val_loader and collect flat lists of preds & labels
with torch.no_grad():
    for imgs, gts in tqdm(val_loader, desc="Evaluating"):
        imgs = imgs.to(device)
        # Binarize masks: anything >0 becomes 1, else 0
        gts_bin = (gts > 0).long().to(device)          # shape [B, H, W]

        preds = model(imgs)
        threshold = 0.55  # or try 0.7 for stricter cloud detection
        preds_bin = (preds > threshold).long().squeeze(1) # shape [B, H, W]

        all_preds.extend(preds_bin.cpu().numpy().ravel())
        all_labels.extend(gts_bin.cpu().numpy().ravel())

# 3) Compute metrics
dice = 2.0 * np.sum(np.array(all_preds) * np.array(all_labels)) / \
       (np.sum(all_preds) + np.sum(all_labels) + 1e-8)
iou   = jaccard_score(all_labels, all_preds, average='binary')
acc   = accuracy_score(all_labels, all_preds)
prec  = precision_score(all_labels, all_preds)
rec   = recall_score(all_labels, all_preds)

print("\nüìä Validation Metrics:")
print(f"  Dice Score:  {dice:.4f}")
print(f"  IoU:          {iou:.4f}")
print(f"  Accuracy:     {acc:.4f}")
print(f"  Precision:    {prec:.4f}")
print(f"  Recall:       {rec:.4f}")

# 4) Visualize up to 3 cloudy patches (skip empty‚Äêmask examples)
count = 0
with torch.no_grad():
    for imgs, gts in val_loader:
        imgs = imgs.to(device)
        gts = gts.to(device)

        for i in range(imgs.size(0)):
            # skip clear patches
            if (gts[i] == 0).all():
                continue

            # Prepare input RGB composite
            img = imgs[i].cpu().numpy()         # [4, H, W]
            rgb = np.stack([img[2], img[1], img[0]], axis=-1)
            rgb = np.clip(rgb, 0, 1)

            # Ground truth & prediction
            gt = (gts[i] > 0).cpu().numpy()
            pred = model(imgs[i:i+1])[0,0].cpu().numpy()
            pred = (pred > 0.5).astype(np.uint8)

            # Plot them
            fig, ax = plt.subplots(1, 3, figsize=(12, 4))
            ax[0].imshow(rgb)
            ax[0].set_title("RGB Input")
            ax[0].axis("off")

            ax[1].imshow(gt, cmap="gray")
            ax[1].set_title("Ground Truth")
            ax[1].axis("off")

            ax[2].imshow(pred, cmap="gray")
            ax[2].set_title("Prediction")
            ax[2].axis("off")

            plt.tight_layout()
            plt.show()

            count += 1
            if count >= 3:
                break
        if count >= 3:
            break
            send_telegram("‚úÖ step 5 completed!")

"""plot loss curves"""

import matplotlib.pyplot as plt

# Full list of training losses from epochs 1 to 60
train_losses = [
    0.3460, 0.3143, 0.3066, 0.2859, 0.2760, 0.2611, 0.2432, 0.2318, 0.2380, 0.2154,
    0.2164, 0.2086, 0.2054, 0.1984, 0.1886, 0.1971, 0.1794, 0.1946, 0.1674, 0.1654,
    0.1639, 0.1666, 0.1644, 0.1635, 0.1614, 0.1559, 0.1501, 0.1486, 0.1395, 0.1460,
    0.1457, 0.1591, 0.1534, 0.1569, 0.1515, 0.1478, 0.1441, 0.1406, 0.1424, 0.1398,
    0.1341, 0.1313, 0.1348, 0.1368, 0.1361, 0.1360, 0.1309, 0.1263, 0.1256, 0.1255,
    0.1253, 0.1229, 0.1220, 0.1261, 0.1162, 0.1269, 0.1164, 0.1240, 0.1158, 0.1195
]

# Full list of validation losses
val_losses = [
    0.2735, 0.2828, 0.2730, 0.2493, 0.2152, 0.2206, 0.1959, 0.2195, 0.2017, 0.1907,
    0.2167, 0.2553, 0.1854, 0.1704, 0.2102, 0.1741, 0.1968, 0.1797, 0.1400, 0.1417,
    0.1555, 0.1438, 0.1395, 0.1799, 0.1453, 0.1400, 0.1507, 0.1386, 0.1341, 0.1460,
    0.1326, 0.1304, 0.1283, 0.1850, 0.1669, 0.1293, 0.1725, 0.1265, 0.1291, 0.1266,
    0.1289, 0.1214, 0.1280, 0.1301, 0.1319, 0.1493, 0.1131, 0.1117, 0.1182, 0.1223,
    0.1135, 0.1169, 0.1232, 0.1239, 0.1202, 0.1202, 0.1084, 0.1156, 0.1179, 0.1187
]

# Mark best epoch
best_epoch = val_losses.index(min(val_losses))
best_val   = min(val_losses)

# Plot
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label="Train Loss", color='blue')
plt.plot(val_losses,   label="Val Loss",   color='orange')
plt.scatter(best_epoch, best_val, color='red', label=f'Best Val (Epoch {best_epoch+1})')

plt.title("Training vs Validation Loss (Epochs 1‚Äì60)")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.grid(True)
plt.legend()
plt.tight_layout()

# Save it
plt.savefig('/content/drive/MyDrive/95Clouds/loss_curve_1_60.png', dpi=300)
plt.show()

"""Threshold sweep - find best probability setup"""

# üîÅ Balanced 5-threshold Sweep (efficient & accurate) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
torch.cuda.empty_cache()
val_loader = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=0)

thresholds = [0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70]
results = {}

model.eval()

with torch.no_grad():
    for th in thresholds:
        print(f"\n‚ñ∂ Threshold {th:.2f}")
        inter_total, union_total, dice_total, count = 0, 0, 0, 0
        for imgs, gts in tqdm(val_loader, desc=f"TH={th:.2f}", leave=False):
            imgs = imgs.to(device)
            gts  = (gts > 0).long().to(device)
            preds = (model(imgs) > th).long().squeeze(1)

            for i in range(preds.size(0)):
                p = preds[i].cpu().numpy().ravel()
                t = gts[i].cpu().numpy().ravel()
                inter = np.sum(p * t)
                union = np.sum(np.clip(p + t, 0, 1))
                dice  = 2 * inter / (np.sum(p) + np.sum(t) + 1e-8)
                inter_total += inter
                union_total += union
                dice_total  += dice
                count += 1

        avg_dice = dice_total / count
        avg_iou  = inter_total / (union_total + 1e-8)
        results[th] = (avg_dice, avg_iou)

# Print sorted results
print("\n‚ö° Threshold Sweep Results (Sorted by Dice):")
for th, (d, i) in sorted(results.items(), key=lambda x: x[1][0], reverse=True):
    print(f"TH={th:.2f} | Dice={d:.4f} | IoU={i:.4f}")

send_telegram("‚úÖ threshold finished!")

"""Inference test dataset - no masks to compare"""

# ‚îÄ‚îÄ‚îÄ Inference Dataset for 38‚ÄëCloud_test patches ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class LandsatTestPatches(Dataset):
    """
    Loads blue/green/red/nir patches from 38‚ÄëCloud_test folders.
    No masks are returned.
    """
    def __init__(self, root_dir):
        base = os.path.join(root_dir, "38-Cloud_test")
        self.band_dirs = {b: os.path.join(base, f"test_{b}")
                         for b in ("blue","green","red","nir")}
        # Collect common suffixes
        sets = []
        for b,p in self.band_dirs.items():
            names = [f for f in os.listdir(p) if f.startswith(f"{b}_patch_")]
            sets.append({n.split(f"{b}_patch_")[1] for n in names})
        self.files = sorted(set.intersection(*sets))
        print(f"‚Üí {len(self.files)} inference patches found")

    def __len__(self): return len(self.files)

    def __getitem__(self, idx):
        suffix = self.files[idx]
        bands = []
        for b,p in self.band_dirs.items():
            img = Image.open(os.path.join(p, f"{b}_patch_{suffix}"))
            bands.append(np.array(img, dtype=np.float32))
        image = np.stack(bands, axis=0) / 10000.0
        return suffix, torch.from_numpy(image)          # return suffix to locate patch

# ‚îÄ‚îÄ‚îÄ Example usage: run model & save predictions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
root = "/content/drive/MyDrive/95Clouds"
test_ds = LandsatTestPatches(root)
test_loader = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=2)

model.eval()
threshold = 0.55   # (use best from sweep)

save_dir = "/content/drive/MyDrive/95Clouds/test_preds"
os.makedirs(save_dir, exist_ok=True)

with torch.no_grad():
    for suffixes, imgs in tqdm(test_loader):
        imgs = imgs.to(device)
        preds = (model(imgs) > threshold).squeeze(1).cpu().numpy()
        for suf, pr in zip(suffixes, preds):
            # pr is [H,W] binary mask (0/1); save as 8‚Äëbit PNG
            Image.fromarray((pr*255).astype(np.uint8)).save(
                os.path.join(save_dir, f"pred_{suf.replace('.TIF','.png')}"))
print("‚úÖ inference", save_dir)
send_telegram("‚úÖ inference finished")

"""Test set overlay mosaic"""

import os, re
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

def stitch_patches(pred_dir, output_path, scene_prefix, patch_size=384):
    """
    Dynamically sizes the canvas to the max row/col indices found in filenames,
    then stitches patches accordingly.
    """
    # 1) Gather all valid (y,x) coords for this scene
    coords = []
    for fname in os.listdir(pred_dir):
        if not fname.startswith("pred_") or scene_prefix not in fname:
            continue
        m = re.search(r"pred_\d+_(\d+)_by_(\d+)_", fname)
        if not m:
            print(f"‚ö†Ô∏è Skipping malformed: {fname}")
            continue
        y, x = int(m.group(1)), int(m.group(2))
        coords.append((y, x))
    if not coords:
        print(f"‚ö†Ô∏è No patches found for scene {scene_prefix}")
        return None

    # 2) Determine canvas size
    max_y = max(y for y,x in coords)
    max_x = max(x for y,x in coords)
    height = (max_y + 1) * patch_size
    width  = (max_x + 1) * patch_size

    canvas = np.zeros((height, width), dtype=np.uint8)
    placed = 0

    # 3) Place each patch
    for fname in os.listdir(pred_dir):
        if not fname.startswith("pred_") or scene_prefix not in fname:
            continue
        m = re.search(r"pred_\d+_(\d+)_by_(\d+)_", fname)
        if not m:
            continue
        y, x = int(m.group(1)), int(m.group(2))

        patch_img = np.array(Image.open(os.path.join(pred_dir, fname)))
        if patch_img.shape != (patch_size, patch_size):
            print(f"‚ö†Ô∏è Wrong shape {patch_img.shape}, skipping {fname}")
            continue

        rs, cs = y*patch_size, x*patch_size
        canvas[rs:rs+patch_size, cs:cs+patch_size] = patch_img
        placed += 1

    # 4) Save & return
    if placed:
        Image.fromarray(canvas).save(output_path)
        print(f"‚úÖ Mosaic saved: {output_path} ({placed} patches at {height}√ó{width})")
        return canvas
    else:
        print(f"‚ö†Ô∏è No valid patches placed for {scene_prefix}")
        return None

def overlay_prediction_on_rgb(rgb_path, pred_mask, output_path, alpha=0.4):
    """
    Overlay mask onto the RGB scene and save.
    """
    rgb = Image.open(rgb_path).convert("RGB")
    rgb = rgb.resize(pred_mask.shape[::-1])

    plt.figure(figsize=(8,8))
    plt.imshow(rgb)
    plt.imshow(pred_mask, cmap='Reds', alpha=alpha)
    plt.axis('off')
    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()
    print(f"üñºÔ∏è Overlay saved: {output_path}")

def overlay_prediction_on_rgb(rgb_path, pred_mask, alpha=0.4):
    rgb = Image.open(rgb_path).convert("RGB")
    rgb = rgb.resize(pred_mask.shape[::-1])  # resize to match mask

    plt.figure(figsize=(10, 10))
    plt.imshow(rgb)
    plt.imshow(pred_mask, cmap='Reds', alpha=alpha)
    plt.title("Overlay: Predicted Cloud Mask on RGB")
    plt.axis("off")
    plt.show()

# Setup
pred_dir = "/content/drive/MyDrive/95Clouds/test_preds"
rgb_base = "/content/drive/MyDrive/95Clouds/38-Cloud_test/Natural_False_Color"
save_dir = "/content/drive/MyDrive/95Clouds/mosaics"
os.makedirs(save_dir, exist_ok=True)

# List of scene IDs (manually collected from your filenames)
scene_ids = [
    "LC08_L1TP_029032_20160720_20170222_01_T1",
    "LC08_L1TP_029044_20160720_20170222_01_T1",
    "LC08_L1TP_035029_20160120_20170224_01_T1",
    "LC08_L1TP_035035_20160120_20170224_01_T1",
    "LC08_L1TP_029041_20160720_20170222_01_T1",
    "LC08_L1TP_034037_20160520_20170223_01_T1",
    "LC08_L1TP_003052_20160120_20170405_01_T1"
]

# Run mosaic + overlay for each scene
for scene_id in scene_ids:
    mask_path = os.path.join(save_dir, f"mask_{scene_id}.png")
    overlay_path = os.path.join(save_dir, f"overlay_{scene_id}.png")
    rgb_path = os.path.join(rgb_base, f"{scene_id}.TIF")

    mask = stitch_patches(pred_dir, mask_path, scene_id)
    if mask is not None:
        overlay_prediction_on_rgb(rgb_path, mask, overlay_path)

import torch
# Rebuild + load your trained model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = UNet4(in_ch=4, base_c=32).to(device)
model.load_state_dict(torch.load("/content/drive/MyDrive/95Clouds/best_unet_model.pth"))
model.eval()

# Dummy input for ONNX export
dummy_input = torch.randn(1, 4, 384, 384).to(device)

# Export to ONNX
onnx_path = "/content/drive/MyDrive/95Clouds/unet95_final.onnx"
torch.onnx.export(
    model, dummy_input, onnx_path,
    export_params=True,
    opset_version=12,
    do_constant_folding=True,
    input_names = ['input'],
    output_names = ['output']
)
print(f"‚úÖ Exported ONNX model to {onnx_path}")